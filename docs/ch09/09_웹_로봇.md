# 09. 웹 로봇

- 웹 로봇은 사람과 상호 작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 그 방식에 따라 `크롤러`, `스파이더`, `웜`, `봇` 등 다양하게 불린다

## 9.1 크롤러와 크롤링

- 웹 크롤러는 웹 페이지를 한 개 가져오고, 그 페이지가 가리키는 모든 웹페이지를 다시 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
- HTML 로 만들어진 웹을 따라 `기어다니기(crawl)` 때문

### 9.1.1 어디에서 시작하는가: '루트 집합'

- 크롤러가 방문을 시작하는 초기 집합을 루트 집합(root set)이라고 부르며, 적절한 조합을 바탕을 최적의 루트 수를 통해 모든 페이지를 크롤링하는 케이스가 좋다

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러들은 크롤링하는 HTML 파싱을 통해 이들 링크를 추출하고 상대 링크를 절대 링크로 변환할 필요가 있다

### 9.1.3 순환 피하기

- 로봇들은 순환을 피하기 위해서 그들이 어디를 방문했는지 반드시 기록해야 한다

### 9.1.4 루프와 중복

- 순환이 발생하면 크롤러 자체가 루프에 빠져서 동일한 결과만 반복(loops)하거나, 동일한 결과물(dups)만 생성하며 서버에 부담을 주는 문제를 발생 시킨다

### 9.1.5 빵 부스러기의 흔적

- 순환을 피하기 위해서는 효율적인 URL 목록 자료 구조가 필요하며, 해당 자료 구조는 빠른 검색 속도와 메모리 효율성이 요구된다

#### 트리와 해시 테이블
- 빠른 검색을 위해서 트리 O(logN ~ N) 혹은 해시 테이블 O(1 ~ N) 을 사용

#### 느슨한 존재 비트맵
- 공간 활용을 최소화하기 위해 대규모 크롤러들은 존재 비트 배열(presence bit array)과 가은 느슨한 자료 구조를 사용
- URL 은 해시 함수에 의해 고정된 크기 숫자로 변환, 해당 숫자 배열에 대응하는 존재 비트를 가지고 해당 값을 바탕으로 빠른 검색 및 크롤링 여부를 간주

> 느슨한 존재 비트맵 예시
> - 느슨한 존재 비트맵을 활용한 URL 크롤링 알고리즘의 간단 예시
>   - 약 3개의 hash 함수(h1, h2, h3)를 준비, 해당 hash 함수는 특정 url 을 0 ~ 9 의 숫자로 변환하는 함수라고 가정
>   - http://naver.com 을 각각의 h1, h2, h3 를 통과 -> 2, 7, 9 라는 결과를 얻음
>   - 인덱스 2, 7, 9 에 해당하는 비트를 1 로 설정 -> 0 0 1 0 0 0 0 1 0 1
>   - http://google.com 을 각각의 해시 함수를 통과 -> 2, 4, 8 이라는 결과를 얻음 -> 인덱스 2, 4, 8 에 해당하는 비트를 확인 -> 2 는 겹치지만 4, 8 은 겹치지 않음 -> 해당 URL 은 검색이 안되었음을 가정하고 크롤링
>   - 4, 8 번째 비트 업데이트 -> 0 0 1 1 0 0 0 1 1 1
> - 이러한 방식으로 URL 을 준비한 해시 함수로 변환한 결과와 비트에 저장된 값이 전부 1인 케이스를 제외하고 크롤링을 진행
> - 기존 true/false 를 저장하는 제일 작은 데이터 타입인 Boolean 타입이 1byte(8bit) 를 차지하는 반면 존재 비트맵은 1bit 만 사용하므로 8배의 공간을 절약 가능
> - 해당 url 을 방문 했는지 안했는지를 정확하게 파악은 불가능하지만, 방문을 2번하는 케이스는 명확하게 걸러낼 수 있으므로 효율적
> - 비트 연산을 통한 빠른 하드웨어 처리 속도 + O(1) 의 검색 속도로 인하여 일반적인 O(1) 의 케이스 보다 빠른 속도를 보여줌 

#### 체크 포인트
- 크롤링의 갑작스러운 중단을 대비, 방문한 URL 목록을 저장하는지 확인

#### 파티셔닝
- 인터넷은 방대하므로 크롤러는 여러 웹 로봇이 동시에 일하는 `농장(farm)` 을 이용하고, 여러 로봇은 인터넷의 특정 부분을 할당하고 해당 부분을 책임지는 형태로 수행

### 9.1.6 별침(alias)과 로봇 순환

- URL 이 별칭을 가질 수 있는 이상 다른 URL 이라도 같은 리소스를 가르킬수 있다

![img.png](images/09_web_robot_01.png)

### 9.1.7 URL 정규화 하기

- 대부분의 웹 로복은 URL 들을 표준 형식으로 '정규화 하여, 다른 URL 이 같은 리소스를 가르키고 있는 것들을 제거하려 시도한다
  - 포트가 없는 경우면 `:80` 포트 추가
  - 이스케이핑된 문자는 대응되는 문자로 추가
  - `#` 태그 제거
- 하지만 이러한 정규화를 통해서도 제거할 수 없는 URL 별칭이 존재

### 9.1.8 파일 시스템의 링크 순환

- 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서 끝없이 깊은 디렉터리 계층을 만들 수 있으며, 악의적으로 활용되기도 한다

![img.png](images/09_web_robot_02.png)

- (b) 의 케이스는 subdir 은 위쪽을 가르키는 심벌릭 링크이지만 URL 은 다르게 보이기 때문에 웹 로봇에게 루프를 일으킬 수 있다

### 9.1.9 동적 가상 웹 공간

- 악의적인 웹 마스터들은 가상의 URL 을 포함한 HTML 을 임의로 생성하여 웹 로봇에게 순환을 일으킬 수 있다
- 물론 악의적 웹 마스터가 아니더라도 달력을 만들어주는 페이지의 경우 웹 로봇이 무한히 다음 달을 누르며 순회하는 문제를 일으킬 가능성을 제공 가능하다

### 9.1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법은 없으므로, 웹 로봇들은 순환을 피하기 위해 아래와 같은 휴리스틱 집합을 필요로 한다

#### URL 정규화
- URL 을 표준 형태로 변환하여, 같은 리소스를 가르키는 다른 URL 이 생기는 것을 일부 회피

#### 너비 우선 크롤링
- URL 의 집합을 순회할 때 너비 우선으로 탐색을 스케쥴링하여 순환이 발생하는 확률을 줄이는 방법

#### 스로틀링
- 일정 시간 동안 가져올 수 있는 페이지 숫자를 제한하거나, 특정 서버에 대한 접근 횟수에 대한 제한을 통해 해결

#### URL 크기 제한
- 순환으로 계속 길어지는 URL 을 차단하기 위해 일정 크기(1KB) 이상의 URL 은 크롤링을 거부
- 대신 해당 휴리스틱의 적용으로 못 가져오는 문서가 발생할 가능성 존재
- 크기 제한을 걸고, 해당 크기 이상의 URL 이 발생하면 해당 에러 로그를 남김으로서 보완이 가능

> 현재의 URL 에서 1KB 는 몇 글자일까요?
> - ASCII 문자는 1글자 - 1Byte
> - 다국어가 들어가면 UTF-8 인코딩에의해 1글자 - 3Byte
> - 따라서, 341 ~ 1024 글자 사이

#### URL/사이트 블랙 리스트
- 문제나 순환을 일으키는 악의적인 사이트를 발견할 때 마다 블랙리스트에 추가하고 해당 블랙 리스트를 제외하고 크롤링

#### 패턴 발견
- 순환으로 인한 URL 상의 반복 패턴을 감지하여 크롤링을 거절

#### 콘텐츠 지문(Fingerprint)
- 크롤링 된 페이지에서 몊 바이트를 얻어내어 체크섬을 계산하고, 이후에 동일한 체크섬을 가진 페이지가 발견되면 해당 링크는 크롤링을 제한

#### 사람의 모니터링
- 아무리 좋은 휴리스틱을 적용해도 사람이 직접 확인하는 모니터링을 통해 의존하는 편이 좋다

## 9.2 로봇의 HTTP

- 로봇들도 다른 HTTP 클라이언트과 동일하며, HTTP/1.1 을 사용할 경우 복잡한 HTTP 헤더를 직접 만들고 요청해야하는 만큼 많은 로봇들은 HTTP/1.0 을 사용

### 9.2.1 요청 헤더 식별하기

- 로봇들은 서버에게 자신의 신원을 식별하기 위해 아래와 같은 헤더를 전달하는 편이 좋다
- User-Agent
  - 서버에게 로봇의 이름을 전달
- From
  - 로봇의 사용자/관리자의 이메일 주소를 제공
- Accept
  - 서버에게 어떤 미디어 타입을 보내도 되는지 전달
- Referer
  - 현재의 요청 URL 을 토함한 문서의 URL 을 제공

### 9.2.2 가상 호스팅

- 가상 호스팅이 널리 퍼진 웹 환경에서 로봇 구현자들은 Host 헤더를 지원할 필요가 있으며, 헤당 헤더를 지원하지 않을 경우 로봇이 특정 URL 에 대해 잘못된 콘텐츠를 찾게 만들 수 있다
- 하나의 서버에서 2개 이상의 URL 의 서비스를 운영하는 경우, 로봇은 1번 URL 에서 받은 콘텐츠를 2번 URL 에서 받은 것으로 착각할 수 있으며 이는 크롤링 결과에 큰 영향을 미칠 수 있다

### 9.2.3 조건부 요청

- 로봇들이 항상 웹 정보를 전부 받아오는 것은 큰 낭비이므로, 마지막으로 받아간 버전 이후 업데이트가 있는지를 확인하는 조건부 HTTP 요청을 사용할 수 있다

### 9.2.4 응답 다루기

- 조건부 요청과 같은 몇몇 기능이나, 좀 더 고도의 결과를 원하는 로봇의 경우 GET 이외의 여러 종류에 대한 HTTP 응답을 다룰 필요가 있다

#### 상태 코드
- 상태 코드 기반의 응답을 이해할 수 있어야 하며, 서버가 항상 옳은 형태의 코드를 반환하는것이 아닌 만큼 예외에 대한 대비도 필요하다

#### 엔터티
- `http-equiv` 과 같은 HTML 메타 태그는 언터티에 포함 된 리소스를 바탕으로 HTTP 헤더를 덮어 씌울 수 있는 만큼 해당 정보에 대한 파싱 기능이 필요할 수 있다

### 9.2.5 User-Agent 타겟팅

- 사이트 관리자들은 로봇에 대한 응답도 고려하여 로봇이 서버에서 아무것도 얻지 못하는 상황에 대비가 필요

## 9.3 부적절하게 동작하는 로봇들

#### 폭주하는 로봇
- 로봇이 논리적인 에러로 인해 순환에 빠지는 경우 서버에 극심한 부담을 초래하므로, 로봇 개발자는 이러한 상황에 고려하여 설계해야 한다

#### 오래된 URL
- 몇몇 로봇들은 URL 목록을 따라 방문하므로, 오래되거나 삭제된 URL 에 대한 관리가 필요

#### 길고 잘못된 URL
- 순환이나 개발상의 오류로 크고 의미 없는 URL 에 방문하는 경우 서버에 부하, 로그 혼선, 에러 유발이 가능하므로 주의 필요

#### 호기심이 지나친 로봇
- 서버에서 노출하기 원하지 않는 민감한 정보를 요구하는 로봇은 큰 문제를 일으킬 수 있으므로 이에대한 설계가 필요

#### 동적 게이트웨이 접근
- 로봇은 게이트웨이 어플리케이션의 콘텐츠에 대한 URL 요청이 가능하지만, 해당 데이터의 경우 특수 목적을 위한 것일 확률이 높고 처리 비용이 많이 들게 된다. 따라서 이에 대한 고려도 필요

## 9.4 로봇 차단하기

- 1994년 로봇에 의해 서버에서 원치 않은 동작을 하거나 민감한 정보에 접근하는 등의 문제가 생길 수 있으므로 `Robots Exclusion Standard` 라는 표준이 만들어짐. 현재는 `robots.txt` 로 통용
- 로봇은 서버에서 우선 해당 서버의 `robots.txt` 를 요청하고 해당 문서에 작성된 권한을 기반으로 작업을 수행

![img.png](images/09_web_robot_04.png)

### 9.4.1 로봇 차단 표준

- 로봇 차단 표준은 임시 방편으로 마련된 표준이며, 업체들이 작성한 표준의 부분 집합을 사용한다
-  v2.0 은 복잡함의 문제로 대부분은 v0.0 이나 v1.0 을 채택

![img.png](images/09_web_robot_03.png)

> 현 시점에서도 아직 공식 표준은 없지만 robots.txt 를 사용하는 REP(Robots Exclusion Protocol)가 구글, 마이크로소프트, 야후 등 주요 검색 엔진 및 크롤러 제작사들이 사용하면서 사실상의 표준(De facto standard)으로 자리 잡음

### 9.4.2 웹사이트와 robots.txt 파일들

- 로봇이 웹 어떤 URL 을 방문하기 전에 해당 사이트의 `robots.txt` 파일을 확인 후, 해당 파일을 처리해야만 한다
- 가상 호스팅 상황에서 각각 도메인에 따른 `robots.txt` 를 제공하는 방법은 책이 쓰여진 시점에는 존재하지 않았으므로 서버 관리자는 호스팅 되는 모든 도메인을 위한 종합적 `robots.txt` 를 작성할 책임이 존재

> 현재는 NginX 등을 리버스 프록시로 사용하므로 해당 서버 설정을 통해 도메인별로 상이한 robots.txt 제공이 가능
```shell
# 도메인 A 에 대한 설정
server {
listen 80;
server_name domainA.com www.domainA.com;
root /var/www/domainA;
    location = /robots.txt {
        # domainA에 특화된 robots.txt 파일을 제공
        alias /etc/nginx/robots/domainA_robots.txt; 
    }
}

# 도메인 B 에 대한 설정
server {
listen 80;
server_name domainB.com www.domainB.com;
root /var/www/domainB;
    location = /robots.txt {
        # domainB에 특화된 robots.txt 파일을 제공
        alias /etc/nginx/robots/domainB_robots.txt;
    }
}
```

#### robots.txt 가져오기
- 로봇은 웹 서버의 다른 리소스와 마찬가지로 HTTP GET 메서드를 이용 `robots.txt` 를 요청
- `robots.txt` 이 있으면 해당 내용을 바탕으로 작업을 수행
- `robots.txt` 이 없으면 404 Not Found 등의 응답을 받고 로봇에 설정된 작업을 수행

```http
GET /robots.txt HTTP/1.0
Host: www.gg.com
User-Agent: Slurp/2.0
Date: Wed Oct 3 20:22:48 EST 2001
```

> Slurp/2.0 은 야후에서 만든 검색 엔진 인덱싱 로봇의 이름

#### 응답 코드
- 웹 서버는 로봇의 `robots.txt` 요청에 응답해야 하며 아래와 같은 액션을 가진다
- 서버에 `robots.txt` 가 존재 : 20X 응답 / `robots.txt` 의 규칙을 바탕으로 작업을 수행
- 서버에 `robots.txt` 가 없을 때 : 404 응답 / `robots.txt` 제약 없이 작업을 수행
- 서버에서 401 또는 403 응답 : 크롤링에 대한 접근 권한이 완전히 제한 -> 작업 수행 불가능
- 서버에서 50X 응답 : 현재 서버에 대한 작업은 미뤄야 함
- 서버에서 30X 응답 : 리소스가 발견 될 때 까지 리다이렉트를 수행

### 9.4.3 robots.txt 파일 포맷

- `robots.txt` 파일은 매우 단순한 줄 기반 문법을 가지며, 각 줄은 빈줄 / 주석 줄 / 규칙 줄로 구성 된다

```
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private
Allow: /public

User-Agent: *
Disallow:
```

#### Disallow/Allow 접두 매칭(Prefix Matching)
- Disallow/Allow 의 경우 접두 매칭을 사용하며, 접두어가 같다면 나머지는 전부 동일한 것으로 취급한다

![img.png](images/09_web_robot_05.png)

### 9.4.4 그 외에 알아둘 점

- 크롤러는 `robots.txt` 에서 자신이 이해하지 못하는 필드는 무시
- 하위 호환을 위해 한 줄을 여러 줄로 나누는 것은 금지
- 주석은 파일 어디서든 허용
- `v0.0` 버전의 경우 Allow 를 지원하지 않아, `v0.0` 버전의 로봇은 Allow 를 무시

### 9.4.5 robots.txt 의 캐싱과 만료

- `robots.txt` 을 매일 가져오는 것은 부하가 일어나므로 로봇은 주기적으로 `robots.txt` 가져와서 캐시해야 하며, HTTP 헤더의 캐시 매커니즘을 따른다

### 9.4.6 로봇 차단 Perl 코드

### 9.4.7 HTML 로봇 제어 META 태그

- 로봇 차단 태그를 HTML META 태그를 이요하여 구현이 가능

#### 로봇 META 지시자

```html
<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
```

- NOINDEX : 로봇에게 해당 페이지를 처리하지 말고 무시하기를 지정
- NOFOLLOW : 로봇에게 해당 페이지가 링크한 페이지를 크롤링 하지 말기를 지정
- INDEX : 해당 페이지의 콘텐츠를 인덱싱 가능
- FOLLOW : 해당 페이지에 존재하는 링크 크롤링 가능
- NOARCHIVE: 해당 페이지 캐시를 위한 로컬 사본 생성 불가능
- ALL : INDEX + FOLLOW
- NONE : NOINDEX + NOFOLLOW

#### 검색엔진 META 태그

![img.png](images/09_web_robot_06.png)

## 9.5 로봇 에티켓

- 1993년 웹 로봇 커뮤니티의 개척자인 마틴 코스터는 웹 로봇을 위한 가이드 라인 목록을 작성

- 로봇을 위한 가이드라인 요약
- 신원 식별
   - 로봇의 신원을 밝혀라
   - 기계의 신원을 밝혀라
   - 연락처를 밝혀라
- 동작
  - 긴장하라 : 로봇이 안정화 될 때까지 1년 정도 감시 필요
  - 대비하라 : 로봇 사용을 조직에 알려라
  - 감시와 로그 : 정상적인 동작을 위해 감시 및 로그가 필요
  - 배우고 조정하라 : 크롤링 과정을 통해 배우고 더 알맞게 조정 필요
- 스스로를 제한하라
  - URL을 필터링 하라 : URL 목록에서 이해할 수 없거나 불필요한 것은 필터링 필요
  - 동적 URL을 필터링 하라 : 게이트웨이로 부터의 동적 컨텐츠 크롤링은 불필요
  - Accept 관련 헤더로 필터링 : 로봇은 서버에게 Accept 헤더로 어떤 컨텐츠를 원하는지 알려야 한다
  - robots.txt 를 준수하라
  - 스스로를 억제하라 : 방문한 URL 을 관리하고 너무 자주 방문하지 않도록 조절 필요
- 루프와 중복을 견뎌내기, 그 외의 문제들
  - 모든 응답 코드 다루기 : 가급적 서버의 다양한 HTTP 응답 코드에 대응이 가능해야 함
  - URL 정규화 하기 : 모든 URL 을 표준화 & 정규화 하여 중복을 제거
  - 적극적으로 순환 피하기
  - 함정을 감시 : 악의적 사이트나 순환이 가능한 부분을 피하기
  - 블랙리스트를 관리하라
- 확장성
  - 공간 이해하기 : 로봇이 사용할 리소스를 미리 계산하고 동작 시킬 것
  - 대역폭 이해하기 : 사용 가능한 네트워크 대역폭 및 로봇이 사용하는 대역폭 확인 하기
  - 시간 이해하기 : 로봇의 작업에 필요한 시단 이해하고 검사하기
  - 분할 정복 : 대규모 작업의 경우 분할 정복 기법 사용
- 신뢰성
  - 철저하게 테스트하라 : 내부에서 철저하게 테스트 후 외부에 사용
  - 체크 포인트 : 진행 상황을 위한 체크포인트와 재시작 기능을 처음부터 설계할 것
  - 실패에 대한 유연성 : 실패에 대비
- 소통
  - 준비하라 : 크롤링 당하는 사람들로 부터의 문의에 대비
  - 이해하라 : 크롤링 당하는 사람들이 웹에 전문적인 지식이 없을 수 있음을 이해
  - 즉각 대응하라 : 문제에 대해서 즉각 대응하여 사람들로 부터의 문의 가능성을 제거

## 9.6 검색 엔진

- 웹 로봇을 가장 광범위하게 사용한 것은 검색 엔진

### 9.6.1 넓게 생각하라

- 웹의 초창기에는 웹 상의 정적 리소스의 위치를 알려주는 것은 간단한 데이터 베이스로 해결이 가능
- 대신 웹이 커짐으로서 웹 크롤러의 작업은 기하급수적으로 늘어났으며, 작업을 완료하기 위한 시간도 같이 늘어남

### 9.6.2 현대적인 검색 엔진의 아키텍쳐

- 현대적인 검색 엔진들은 `풀 텍스트 색인(Full-Text Indexes)` 이라는 데이터 베이스를 기반으로 검색을 지원
- 문서를 단어 단위로 `토큰화` 하여 분리한 다음, 역색인 작업을 통해 해당 `토큰` 이 어디에 있는지를 인덱싱한 다음 검색 결과 질의에 해당하는 결과를 리턴
- 검색 결과는 관련도 랭킹을 가지며 해당 랭킹에 따라 검색 결과가 정렬 되어 표시

![img.png](images/09_web_robot_08.png)

### 9.6.3 풀 텍스트 색인

### 9.6.4 질의 보내기

### 9.6.5 검색 결과를 정렬하고 보여주기

### 9.6.6 스푸핑(Spoofing)

- 몇몇 서비스 제공자들은 검색 엔진의 상위에 노출되기 위해 크롤러에게만 비정상적인 컨텐츠를 제공하는 스푸핑을 활용하므로, 해당 속임수를 잡아내기 위해서 끊임 없는 개선이 필요