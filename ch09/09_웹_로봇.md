# 09. 웹 로봇

- 웹 로봇은 사람과 상호 작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 그 방식에 따라 `크롤러`, `스파이더`, `웜`, `봇` 등 다양하게 불린다

## 9.1 크롤러와 크롤링

- 웹 크롤러는 웹 페이지를 한 개 가져오고, 그 페이지가 가리키는 모든 웹페이지를 다시 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
- HTML 로 만들어진 웹을 따라 `기어다니기(crawl)` 때문

### 9.1.1 어디에서 시작하는가: '루트 집합'

- 크롤러가 방문을 시작하는 초기 집합을 루트 집합(root set)이라고 부르며, 적절한 조합을 바탕을 최적의 루트 수를 통해 모든 페이지를 크롤링하는 케이스가 좋다

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러들은 크롤링하는 HTML 파싱을 통해 이들 링크를 추출하고 상대 링크를 절대 링크로 변환할 필요가 있다

### 9.1.3 순환 피하기

- 로봇들은 순환을 피하기 위해서 그들이 어디를 방문했는지 반드시 기록해야 한다

### 9.1.4 루프와 중복

- 순환이 발생하면 크롤러 자체가 루프에 빠져서 동일한 결과만 반복(loops)하거나, 동일한 결과물(dups)만 생성하며 서버에 부담을 주는 문제를 발생 시킨다

### 9.1.5 빵 부스러기의 흔적

- 순환을 피하기 위해서는 효율적인 URL 목록 자료 구조가 필요하며, 해당 자료 구조는 빠른 검색 속도와 메모리 효율성이 요구된다

#### 트리와 해시 테이블
- 빠른 검색을 위해서 트리 O(logN ~ N) 혹은 해시 테이블 O(1 ~ N) 을 사용

#### 느슨한 존재 비트맵
- 공간 활용을 최소화하기 위해 대규모 크롤러들은 존재 비트 배열(presence bit array)과 가은 느슨한 자료 구조를 사용
- URL 은 해시 함수에 의해 고정된 크기 숫자로 변환, 해당 숫자 배열에 대응하는 존재 비트를 가지고 해당 값을 바탕으로 빠른 검색 및 크롤링 여부를 간주

> 느슨한 존재 비트맵 예시
> - 느슨한 존재 비트맵을 활용한 URL 크롤링 알고리즘의 간단 예시
>   - 약 3개의 hash 함수(h1, h2, h3)를 준비, 해당 hash 함수는 특정 url 을 0 ~ 9 의 숫자로 변환하는 함수라고 가정
>   - http://naver.com 을 각각의 h1, h2, h3 를 통과 -> 2, 7, 9 라는 결과를 얻음
>   - 인덱스 2, 7, 9 에 해당하는 비트를 1 로 설정 -> 0 0 1 0 0 0 0 1 0 1
>   - http://google.com 을 각각의 해시 함수를 통과 -> 2, 4, 8 이라는 결과를 얻음 -> 인덱스 2, 4, 8 에 해당하는 비트를 확인 -> 2 는 겹치지만 4, 8 은 겹치지 않음 -> 해당 URL 은 검색이 안되었음을 가정하고 크롤링
>   - 4, 8 번째 비트 업데이트 -> 0 0 1 1 0 0 0 1 1 1
> - 이러한 방식으로 URL 을 준비한 해시 함수로 변환한 결과와 비트에 저장된 값이 전부 1인 케이스를 제외하고 크롤링을 진행
> - 기존 true/false 를 저장하는 제일 작은 데이터 타입인 Boolean 타입이 1byte(8bit) 를 차지하는 반면 존재 비트맵은 1bit 만 사용하므로 8배의 공간을 절약 가능
> - 해당 url 을 방문 했는지 안했는지를 정확하게 파악은 불가능하지만, 방문을 2번하는 케이스는 명확하게 걸러낼 수 있으므로 효율적
> - 비트 연산을 통한 빠른 하드웨어 처리 속도 + O(1) 의 검색 속도로 인하여 일반적인 O(1) 의 케이스 보다 빠른 속도를 보여줌 

#### 체크 포인트
- 크롤링의 갑작스러운 중단을 대비, 방문한 URL 목록을 저장하는지 확인

#### 파티셔닝
- 인터넷은 방대하므로 크롤러는 여러 웹 로봇이 동시에 일하는 `농장(farm)` 을 이용하고, 여러 로봇은 인터넷의 특정 부분을 할당하고 해당 부분을 책임지는 형태로 수행

### 9.1.6 별침(alias)과 로봇 순환

- URL 이 별칭을 가질 수 있는 이상 다른 URL 이라도 같은 리소스를 가르킬수 있다

![img.png](images/09_web_robot_01.png)

### 9.1.7 URL 정규화 하기

- 대부분의 웹 로복은 URL 들을 표준 형식으로 '정규화 하여, 다른 URL 이 같은 리소스를 가르키고 있는 것들을 제거하려 시도한다
  - 포트가 없는 경우면 `:80` 포트 추가
  - 이스케이핑된 문자는 대응되는 문자로 추가
  - `#` 태그 제거
- 하지만 이러한 정규화를 통해서도 제거할 수 없는 URL 별칭이 존재

### 9.1.8 파일 시스템의 링크 순환

- 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서 끝없이 깊은 디렉터리 계층을 만들 수 있으며, 악의적으로 활용되기도 한다

![img.png](images/09_web_robot_02.png)

- (b) 의 케이스는 subdir 은 위쪽을 가르키는 심벌릭 링크이지만 URL 은 다르게 보이기 때문에 웹 로봇에게 루프를 일으킬 수 있다

### 9.1.9 동적 가상 웹 공간

- 악의적인 웹 마스터들은 가상의 URL 을 포함한 HTML 을 임의로 생성하여 웹 로봇에게 순환을 일으킬 수 있다
- 물론 악의적 웹 마스터가 아니더라도 달력을 만들어주는 페이지의 경우 웹 로봇이 무한히 다음 달을 누르며 순회하는 문제를 일으킬 가능성을 제공 가능하다

### 9.1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법은 없으므로, 웹 로봇들은 순환을 피하기 위해 아래와 같은 휴리스틱 집합을 필요로 한다

#### URL 정규화
- URL 을 표준 형태로 변환하여, 같은 리소스를 가르키는 다른 URL 이 생기는 것을 일부 회피

#### 너비 우선 크롤링
- URL 의 집합을 순회할 때 너비 우선으로 탐색을 스케쥴링하여 순환이 발생하는 확률을 줄이는 방법

#### 스로틀링
- 일정 시간 동안 가져올 수 있는 페이지 숫자를 제한하거나, 특정 서버에 대한 접근 횟수에 대한 제한을 통해 해결

#### URL 크기 제한
- 순환으로 계속 길어지는 URL 을 차단하기 위해 일정 크기(1KB) 이상의 URL 은 크롤링을 거부
- 대신 해당 휴리스틱의 적용으로 못 가져오는 문서가 발생할 가능성 존재
- 크기 제한을 걸고, 해당 크기 이상의 URL 이 발생하면 해당 에러 로그를 남김으로서 보완이 가능

> 현재의 URL 에서 1KB 는 몇 글자일까요?
> - ASCII 문자는 1글자 - 1Byte
> - 다국어가 들어가면 UTF-8 인코딩에의해 1글자 - 3Byte
> - 따라서, 341 ~ 1024 글자 사이

#### URL/사이트 블랙 리스트
- 문제나 순환을 일으키는 악의적인 사이트를 발견할 때 마다 블랙리스트에 추가하고 해당 블랙 리스트를 제외하고 크롤링

#### 패턴 발견
- 순환으로 인한 URL 상의 반복 패턴을 감지하여 크롤링을 거절

#### 콘텐츠 지문(Fingerprint)
- 크롤링 된 페이지에서 몊 바이트를 얻어내어 체크섬을 계산하고, 이후에 동일한 체크섬을 가진 페이지가 발견되면 해당 링크는 크롤링을 제한

#### 사람의 모니터링
- 아무리 좋은 휴리스틱을 적용해도 사람이 직접 확인하는 모니터링을 통해 의존하는 편이 좋다

